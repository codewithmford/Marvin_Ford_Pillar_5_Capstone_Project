{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## üßπStep 3: Data Preprocessing, EDA & Feature Engineering\n",
        "\n",
        "---\n",
        "This step transforms raw Netflix metadata into a structured, analysis-ready format by addressing data quality issues, exploring catalog imbalances, and engineering content features required for similarity-based recommendation.\n",
        "\n",
        "*   **Approach**: Data cleaning, exploratory analysis, and feature construction\n",
        "*   **Input**: Raw Netflix title metadata\n",
        "*   **Goal**: Prepare reliable content signals for cold-start recommendation\n",
        "*   **Constraint**: No modeling or similarity computation performed"
      ],
      "metadata": {
        "id": "cSnnxGE8KyNZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGfK1ij1KxW2"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# STEP 3: DATA PROCESSING, EDA & FEATURE ENGINEERING\n",
        "# =============================================================================\n",
        "\n",
        "\"\"\"\n",
        "Prepare Netflix title metadata for content-based recommendation by cleaning\n",
        "raw data, exploring key patterns, and creating structured features that\n",
        "support similarity-based modeling.\n",
        "\n",
        "This step directly addresses the data issues identified in Step 2 and\n",
        "translates them into practical preprocessing and feature engineering actions.\n",
        "\n",
        "There were five key problems identified in Step 2, each addressed through\n",
        "a specific subcomponent in this step.\n",
        "\n",
        "Problem: Missing metadata (e.g., cast and director)\n",
        "Solution: 3a. Data Cleaning ‚Äì Missing Value Analysis\n",
        "\n",
        "Problem: Catalog imbalance across content types, genres, and countries\n",
        "Solution: 3b. Exploratory Data Analysis (EDA) ‚Äì Catalog Distributions\n",
        "\n",
        "Problem: Inconsistent text formats across metadata fields\n",
        "Solution: 3c. Text Cleaning and Normalization\n",
        "\n",
        "Problem: Lack of user interaction data for personalization\n",
        "Solution: 3d. Feature Engineering ‚Äì Content Profile Construction\n",
        "\n",
        "Problem: Duplicate title names causing lookup ambiguity\n",
        "Solution: 3e. Duplicate Handling Strategy\n",
        "\"\"\"\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 3a. Data Cleaning ‚Äì Missing Value Analysis\n",
        "# ---------------------------------------------------------------------------\n",
        "# Problem addressed: Missing metadata (e.g., cast and director).\n",
        "# Purpose: Measuring missingness helps decide which fields can still be used\n",
        "# safely without removing otherwise useful titles from the catalog.\n",
        "\n",
        "print(\"DATA CLEANING: MISSING VALUE ANALYSIS\")\n",
        "print(\"\\n\")\n",
        "\n",
        "def analyze_missing(df):                            # Define a reusable function to analyze missing values across all columns.\n",
        "    \"\"\"\n",
        "    Analyze missing values across all columns.\n",
        "\n",
        "    Purpose:\n",
        "    Identify where metadata gaps exist so that preprocessing decisions\n",
        "    remain practical and do not unnecessarily reduce catalog size.\n",
        "    \"\"\"\n",
        "    missing = df.isnull().sum()                     # Count missing values per column.\n",
        "    missing_pct = 100 * missing / len(df)           # Convert missing counts to percentages for severity assessment.\n",
        "\n",
        "    missing_df = pd.DataFrame({                     # Combine counts and percentages into a summary table.\n",
        "        \"Missing Count\": missing,\n",
        "        \"Missing %\": missing_pct\n",
        "    })\n",
        "\n",
        "    return (\n",
        "        missing_df[missing_df[\"Missing Count\"] > 0] # Keep only columns with at least one missing value.\n",
        "        .sort_values(\"Missing %\", ascending=False)  # Sort columns by highest missing percentage.\n",
        "    )\n",
        "\n",
        "missing_summary = analyze_missing(df)               # Apply missing value analysis to the dataset.\n",
        "display(missing_summary)                            # Display missing value summary for review.\n",
        "print(\n",
        "    f\"INTERPRETATION:\\n This table shows that several metadata fields have missing values, \\n \"\n",
        "    f\"with the highest missing rates observed in columns such as cast and director. \\n \"\n",
        "    f\"For example, at least one column exceeds {missing_summary['Missing %'].max():.1f}% missing entries, \\n \"\n",
        "    f\"which indicates that dropping these records would significantly reduce the catalog size. \\n \"\n",
        "    f\"As a result, the recommender is designed to tolerate partial metadata rather than \\n \"\n",
        "    f\"exclude large portions of the dataset. \\n\"\n",
        ")\n",
        "print(\"\\n\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 3b. Exploratory Data Analysis (EDA) ‚Äì Catalog Distributions\n",
        "# ---------------------------------------------------------------------------\n",
        "# Problem addressed: Catalog imbalance across content types and countries.\n",
        "# Purpose: Visualizing these patterns explains potential recommendation bias and\n",
        "# supports the later use of diversity-focused evaluation metrics.\n",
        "\n",
        "print(\"EDA: CATAGLOG DISTRIBUTION\")\n",
        "print(\"\\n\")\n",
        "\n",
        "def plot_distributions(df, columns, figsize=(15, 5)):           # Define a helper function for visualizing distributions.\n",
        "    \"\"\"\n",
        "    Plot distributions for categorical or numerical fields.\n",
        "\n",
        "    Purpose:\n",
        "    Understand how content is distributed across the catalog and identify\n",
        "    dominant patterns that may affect recommendation exposure.\n",
        "    \"\"\"\n",
        "    n_cols = min(3, len(columns))                               # Limit the number of plots per row for readability.\n",
        "    n_rows = (len(columns) + n_cols - 1) // n_cols              # Compute required number of rows dynamically.\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)   # Create subplot grid.\n",
        "    axes = axes.flatten() if n_rows * n_cols > 1 else [axes]    # Flatten axes for consistent indexing.\n",
        "\n",
        "    for idx, col in enumerate(columns):                         # Iterate through selected columns.\n",
        "        if df[col].dtype in [\"int64\", \"float64\"]:               # Check if column is numerical.\n",
        "            axes[idx].hist(df[col].dropna(), bins=30, edgecolor=\"black\")  # Plot histogram for numerical data.\n",
        "        else:\n",
        "            df.loc[df[col] != \"\", col].value_counts().head(10).plot(      # Plot top 10 categories for categorical data.\n",
        "                kind=\"bar\", ax=axes[idx]\n",
        "            )\n",
        "\n",
        "        axes[idx].set_title(col)                                # Set plot title to column name.\n",
        "        axes[idx].tick_params(axis=\"x\", rotation=45)            # Rotate x-axis labels for readability.\n",
        "\n",
        "    plt.tight_layout()                                          # Adjust layout to avoid overlap.\n",
        "    plt.show()                                                  # Display the plots.\n",
        "\n",
        "# Catalog composition: Movies vs TV Shows\n",
        "# ---------------------------------------------------------------------------\n",
        "# Purpose: Understanding the balance between movies and TV shows supports\n",
        "# type-aware recommendation logic and helps avoid mismatched suggestions\n",
        "# like recommending TV series when a user selects a movie.\n",
        "\n",
        "print(\"CATALOG COMPOSITION: MOVIES VS TV SHOWS\")\n",
        "print(\"\\n\")\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "df[\"type\"].value_counts().plot(kind=\"bar\")                      # Plot the count of Movies vs TV Shows in the catalog.\n",
        "plt.title(\"Content Type Distribution (Movies vs TV Shows)\")\n",
        "plt.xlabel(\"Content Type\")\n",
        "plt.ylabel(\"Number of Titles\")\n",
        "plt.show()\n",
        "type_counts = df[\"type\"].value_counts()\n",
        "print(\n",
        "    f\"INTERPRETATION:\\n  This chart shows that the catalog contains {type_counts.get('Movie', 0)} Movies and {type_counts.get('TV Show', 0)} TV Shows. \\n  \"\n",
        "    f\"The higher number of movies indicates a natural imbalance in content availability, \\n  \"\n",
        "    f\"which could bias recommendations toward movies if content type is not considered. \\n  \"\n",
        "    f\"This supports the need for type-aware filtering in later recommendation stages. \\n \"\n",
        ")\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# High-level catalog distributions (Country)\n",
        "# ---------------------------------------------------------------------------\n",
        "# Purpose: Visualizing country dominance highlights exposure imbalance across\n",
        "# regions and reinforces the need for diversity-aware evaluation metrics\n",
        "# such as Catalog Coverage.\n",
        "\n",
        "print(\"COUNTRY DISTRIBUTION\")\n",
        "print(\"\\n\")\n",
        "\n",
        "plot_distributions (df, [\"country\"])                            # Visualize catalog balance by production country.\n",
        "\n",
        "top_country = df[\"country\"].value_counts().idxmax()\n",
        "top_country_count = df[\"country\"].value_counts().max()\n",
        "\n",
        "print(\n",
        "    f\"INTERPRETATION:\\n  The country distribution shows that content production is concentrated \\n  \"\n",
        "    f\"in a small number of regions. The most represented country, {top_country}, \\n  \"\n",
        "    f\"appears {top_country_count} times in the catalog. This concentration increases the risk that \\n  \"\n",
        "    f\"recommendations may repeatedly surface titles from dominant regions, reinforcing the \\n  \"\n",
        "    f\"need for coverage-aware evaluation later in the pipeline. \\n \"\n",
        ")\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# Genre distribution (Top categories)\n",
        "# ---------------------------------------------------------------------------\n",
        "# Purpose: Identifying dominant genres explains the risk of popularity bias and\n",
        "# supports the use of Intra-list Diversity (ILD) as a success metric to\n",
        "# encourage variety in recommendations.\n",
        "\n",
        "print(\"GENRE DISTRIBUTION\")\n",
        "print(\"\\n\")\n",
        "\n",
        "genre_counts = (                                        # Aggregate genre token frequencies.\n",
        "    df[\"listed_in\"]                                     # Access genre metadata field.\n",
        "    .fillna(\"\")                                         # Ensure no NaNs\n",
        "    .str.split(\" \")                                     # Split genre strings into individual tokens.\n",
        "    .explode()                                          # Expand tokens into separate rows for counting.\n",
        "    .loc[lambda x: x.notna() & (x != \"\") & (x != \"&\")]  # Remove blank and non-informative tokens\n",
        "    .value_counts()                                     # Count frequency of each genre token.\n",
        "    .head(10)                                           # Retain only the top 10 most frequent genres.\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "genre_counts.plot(kind=\"bar\")                       # Plot the most common genre tokens.\n",
        "plt.title(\"Top 10 Genre Tokens in Catalog\")\n",
        "plt.xlabel(\"Genre\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "\n",
        "top_genre = genre_counts.index[0]\n",
        "top_genre_count = genre_counts.iloc[0]\n",
        "\n",
        "print(\n",
        "    f\"INTERPRETATION:\\n  This chart shows that a small number of genres dominate the catalog.\\n  \"\n",
        "    f\"The most frequent genre, '{top_genre}', appears {top_genre_count} times among the top tokens.\\n  \"\n",
        "    f\"Such dominance suggests that similarity-based recommendations may repeatedly surface \\n  \"\n",
        "    f\"very similar content unless diversity controls are applied. This directly motivates the \\n  \"\n",
        "    f\"use of Intra-list Diversity as a success metric.\\n \"\n",
        ")\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 3c. Text Cleaning and Normalization\n",
        "# ---------------------------------------------------------------------------\n",
        "# Problem addressed: Inconsistent text formats across metadata fields.\n",
        "# Purpose: Standardizing text ensures similarity calculations focus on meaning\n",
        "# rather than formatting differences.\n",
        "\n",
        "def clean_text(text):                               # Define a utility function for text standardization.\n",
        "    \"\"\"\n",
        "    Clean and normalize text fields.\n",
        "\n",
        "    Purpose:\n",
        "    Standardize text inputs so that similarity calculations are consistent\n",
        "    and comparable across titles.\n",
        "    \"\"\"\n",
        "    if pd.isna(text):                               # Check if the value is missing.\n",
        "        return \"\"                                   # Replace missing values with empty strings.\n",
        "    return (\n",
        "        text.lower()                                # Convert text to lowercase for consistency.\n",
        "        .strip()                                    # Remove leading and trailing whitespace.\n",
        "        .replace(\",\", \" \")                          # Replace commas to standardize token separation.\n",
        "    )\n",
        "\n",
        "text_columns = [\"listed_in\", \"description\", \"cast\", \"director\", \"country\"]  # Define text-based metadata fields.\n",
        "\n",
        "for col in text_columns:                            # Apply text cleaning to each selected column.\n",
        "    df[col] = df[col].apply(clean_text)             # Clean text values in place.\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 3d. Feature Engineering ‚Äì Content Profile Construction\n",
        "# ---------------------------------------------------------------------------\n",
        "# Problem addressed: Lack of user interaction data.\n",
        "# Purpose: Combining metadata fields creates a richer content signal that\n",
        "# supports recommendation in cold-start scenarios.\n",
        "\n",
        "def build_content_profile(row):                     # Define a function to combine metadata into one profile.\n",
        "    \"\"\"\n",
        "    Combine selected metadata fields into a single content profile.\n",
        "\n",
        "    Purpose:\n",
        "    Create a unified text representation that captures genre, theme,\n",
        "    and key attributes needed for similarity-based recommendation.\n",
        "    \"\"\"\n",
        "    return \" \".join([                                # Concatenate relevant metadata fields into one string.\n",
        "        row[\"listed_in\"],\n",
        "        row[\"description\"],\n",
        "        row[\"cast\"],\n",
        "        row[\"director\"],\n",
        "        row[\"country\"].replace(\" \", \"_\") if isinstance(row[\"country\"], str) and row[\"country\"].strip() else \"\"  # Normalize country field to prevent multi-word country names from being treated\n",
        "                                                                                                                # as unrelated tokens during vectorization and similarity computation\n",
        "                                                                                                                # Convert spaces to underscores (e.g., United_States).\n",
        "    ])\n",
        "\n",
        "df[\"content_profile\"] = df.apply(build_content_profile, axis=1)  # Create a content profile for each title.\n",
        "\n",
        "print(\"\\nSample Content Profiles:\\n\")\n",
        "\n",
        "display(df[[\"title\", \"content_profile\"]].head(5))\n",
        "\n",
        "print(\n",
        "    \"\\nINTERPRETATION:\\n\"\n",
        "    \"The content_profile column demonstrates how multiple metadata fields like genres, descriptions, cast, \\n\"\n",
        "    \"director, and country are consolidated into a single unified text representation. This enriched profile \\n\"\n",
        "    \"serves as the foundation for similarity-based modeling, enabling the recommender system to identify \\n\"\n",
        "    \"relationships between titles even in the absence of user interaction data. By combining both thematic \\n\"\n",
        "    \"and categorical signals, the system is better equipped to generate meaningful recommendations in cold-start \\n\"\n",
        "    \"scenarios, where traditional collaborative filtering approaches would not be applicable.\\n\"\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 3e. Duplicate Handling Strategy\n",
        "# ---------------------------------------------------------------------------\n",
        "# Problem addressed: Duplicate title names.\n",
        "# Purpose: Using index-based referencing avoids ambiguity during recommendation\n",
        "# without removing valid titles from the dataset.\n",
        "\n",
        "df = df.reset_index(drop=True)                      # Reset index to ensure unique and stable row identifiers.\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Export Cleaned Dataset\n",
        "# ---------------------------------------------------------------------------\n",
        "# Purpose: Persist the cleaned and feature-engineered dataset for reuse,\n",
        "# reproducibility, and alignment with project structure.\n",
        "\n",
        "processed_path = \"data/processed/netflix_titles_cleaned.csv\"\n",
        "\n",
        "os.makedirs(os.path.dirname(processed_path), exist_ok=True)\n",
        "df.to_csv(processed_path, index=False)\n",
        "\n",
        "print(f\"Cleaned dataset exported to: {processed_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ìÇÉüñä Key Findings\n",
        "\n",
        "Exploratory data analysis identified missing metadata, duplicate title strings, catalog imbalance, and concentration across geography and genres, requiring targeted handling without reducing catalog size. Fully empty columns were removed, while partially missing semantic fields were retained through tolerant preprocessing and feature selection to support cold-start scenarios, and duplicate titles were handled using unique row indices to prevent lookup ambiguity. Catalog imbalance informed type-aware similarity logic, geographic concentration motivated coverage-aware evaluation, and genre dominance justified the use of Intra-List Diversity. These findings guided feature engineering through semantic field selection, normalization, and unified content profile construction, while dimensionality reduction such as PCA was not applicable due to the need to preserve semantic meaning and explainability in text-based similarity modeling.\n",
        "\n",
        "*   **Missing Data**\n",
        "    *  Fully empty placeholder columns: **14 columns** (100% missing, 8,809 rows)\n",
        "    *  Director missing: 29.9% (2,634 titles)\n",
        "    *  Cast missing: 9.4% (825 titles)\n",
        "    *  Country missing: 9.4% (831 titles) - partial metadata retained to preserve catalog coverage\n",
        "    *  How this was addressed:\n",
        "    \t*  Removed fully empty placeholder columns as non-informative noise\n",
        "    \t*  Retained partially missing fields and applied missing-value tolerance rather than row deletion\n",
        "    \t*  Consolidated multiple text fields into a unified content_profile dataframe, allowing similarity to be computed even when some metadata is absent\n",
        "    \t*  This preserves catalog size and ensures robustness in cold-start and incomplete-metadata scenarios.\n",
        "\n",
        "*   **Duplicates**\n",
        "    *  3 duplicate strings found\n",
        "    *  How this was addressed:\n",
        "    \t*  Did not remove duplicate rows to avoid discarding valid titles that share the same name\n",
        "    \t*  Reset and relied on unique row indices for all similarity computation and recommendation lookups\n",
        "    \t*  Enforced deterministic title resolution during recommendation retrieval\n",
        "    \t*  This is to revent lookup ambiguity while preserving catalog completeness and recommendation integrity\n",
        "\n",
        "*   **Catalog Imbalance**\n",
        "    *  Movies: **6,132 titles**\n",
        "    *  TV Shows: **2,677 titles**\n",
        "    *  Supports type-aware filtering\n",
        "    *  How this was addressed:\n",
        "    \t*  Enforced **type-aware similarity logic** so movies are compared only with movies and TV shows with TV shows\n",
        "    \t*  Prevented cross-type recommendations that would reduce relevance\n",
        "    \t*  This reduces structural bias toward movies and improves contextual relevance of recommendations.\n",
        "\n",
        "*   **Geographic Concentration**\n",
        "    *  United States: **2,819 titles** - dominant production region\n",
        "    *  Motivates coverage-aware evaluation\n",
        "    *  How this was addressed:\n",
        "    \t*  Measured **Catalog Coverage (CC)** during model evaluation to track how broadly recommendations surface the catalog\n",
        "    \t*  Applied balanced feature weighting to prevent country metadata from overpowering semantic content\n",
        "    \t*  This limits repeated exposure to dominant regions and supports fairer catalog representation.\n",
        "\n",
        "*   **Genre Dominance**\n",
        "    *  Top genre token **‚ÄúTV‚Äù appears 5,230 times**\n",
        "    *  Indicates heavy concentration in a small number of genres\n",
        "    *  How this was addressed:\n",
        "    \t*  Introduced **Intra-List Diversity (ILD)** as a core evaluation metric\n",
        "    \t*  Tuned genre weighting in hybrid similarity models to balance relevance and variety\n",
        "    \t*  This prevents overly repetitive recommendations and improves perceived discovery value.\n",
        "\n",
        "*   **Feature Engineering & Selection**\n",
        "    *  Selected content signals: **description, listed_in (genres), cast, director, country**\n",
        "    *  Excluded metadata: **release year, rating, duration, date added** (non-semantic)\n",
        "    *  Text normalization: lowercasing, delimiter standardization, missing-value handling\n",
        "    *  Feature construction: merged selected fields into a unified **content_profile** dataframe\n",
        "    *  Dimensionality Reduction: PCA was not applicable to this case because preserving semantic meaning and explainability is essential in text-based similarity modeling.\n",
        "    *  How this was addressed:\n",
        "    \t*  Performed **explicit feature selection at the metadata level** rather than statistical dimensionality reduction\n",
        "    \t*  Preserved full semantic text for TF-IDF and embedding-based similarity\n",
        "    \t*  Avoided PCA to maintain interpretability and traceability of similarity decisions\n",
        "    \t*  This maintains explainability, semantic richness, and alignment with content-based retrieval objectives.\n",
        "\n",
        "*   Output:\n",
        "    *  Cleaned, feature-ready dataset exported for similarity modeling\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "lMlljGyicpt-"
      }
    }
  ]
}