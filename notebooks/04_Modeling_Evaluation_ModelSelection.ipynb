{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ› ï¸ Step 4: Model Building, Hyperparameter Tuning, Evaluation & Final Model Selection\n",
        "\n",
        "---\n",
        "This step evaluates multiple content-based similarity approaches to determine which method best supports relevant, diverse, and explainable recommendations at scale. By comparing rule-based heuristics, TF-IDF representations, weighted hybrid models, and semantic embeddings under a consistent cosine similarity framework, this step surfaces practical trade-offs between relevance, diversity, and catalog exposure. The goal is not to maximize a single metric, but to identify a balanced solution that performs reliably in cold-start and data-limited environments.\n",
        "\n",
        "\n",
        "*   **Approach**: Similarity modeling and comparative evaluation\n",
        "*   **Models**: Rules-based, TF-IDF, weighted hybrid TF-IDF, semantic embeddings\n",
        "*   **Similarity Metric**: Cosine similarity (consistent across models)\n",
        "*   **Goal**: Identify a robust, explainable recommender aligned with Success Metrics"
      ],
      "metadata": {
        "id": "FB0cNvUdK8re"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q07TvFDUK63P"
      },
      "outputs": [],
      "source": [
        "# ==================================================================================\n",
        "# STEP 4: MODEL BUILDING, HYPERPARAMETER TUNING, EVALUATION & FINAL MODEL SELECTION\n",
        "# ==================================================================================\n",
        "\n",
        "\"\"\"\n",
        "Transform engineered content features into numerical representations and\n",
        "compute similarity scores to enable content-based recommendations.\n",
        "\n",
        "This step evaluates multiple content similarity approaches using the\n",
        "engineered content profiles, including rule-based similarity,\n",
        "TF-IDF (Term Frequencyâ€“Inverse Document Frequency), weighted hybrid\n",
        "content similarity, and embedding-based representations. These approaches\n",
        "are tested iteratively to identify the most effective method for producing\n",
        "relevant, diverse, and explainable recommendations.\n",
        "\n",
        "TF-IDF (Term Frequencyâ€“Inverse Document Frequency) represents titles as\n",
        "weighted term vectors that emphasize distinctive content attributes.\n",
        "Weighted hybrid similarity extends this approach by adjusting the\n",
        "influence of selected metadata fields based on observed performance.\n",
        "Embedding-based representations further capture semantic relationships\n",
        "between titles beyond direct term overlap.\n",
        "\n",
        "Cosine similarity is used consistently across all models to quantify\n",
        "item-to-item similarity by comparing vector orientation rather than\n",
        "absolute magnitude, making it well-suited for sparse and dense\n",
        "representations.\n",
        "\n",
        "What this step does:\n",
        "- Converts content profiles into multiple vector representations\n",
        "- Computes similarity scores using rule-based, TF-IDF, hybrid, and\n",
        "  embedding-based approaches\n",
        "- Performs cross-validation and comparative evaluation across models\n",
        "- Summarizes performance metrics and supports final model selection\n",
        "\n",
        "What this step does NOT do:\n",
        "- Train supervised predictive models\n",
        "- Use user interaction or rating data\n",
        "- Conduct online experiments or A/B testing\n",
        "\"\"\"\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 4a. Baseline: Rules Based\n",
        "# ---------------------------------------------------------------------------\n",
        "# Purpose: Provide a simple, fully explainable benchmark using basic content\n",
        "# rules (type + genre overlap). This helps validate that learned similarity\n",
        "# methods add measurable value beyond heuristics.\n",
        "\n",
        "def build_rules_based_similarity(df):                                    # Build heuristic similarity from simple metadata rules.\n",
        "    \"\"\"\n",
        "    Build a rules-based similarity matrix.\n",
        "\n",
        "    Purpose:\n",
        "    Create a benchmark recommender that relies on straightforward rules\n",
        "    (same type and overlapping genres) for comparison against learned models.\n",
        "    \"\"\"\n",
        "    n = df.shape[0]                                                      # Capture number of catalog items for matrix sizing.\n",
        "    sim = np.zeros((n, n), dtype=float)                                  # Initialize dense matrix for transparency and simplicity.\n",
        "\n",
        "    type_series = df[\"type\"].fillna(\"\")                                  # Ensure content type is non-missing for comparisons.\n",
        "\n",
        "    # Normalize genre tokenization\n",
        "    # Purpose: Ensure genre handling is consistent across similarity\n",
        "    # computation and proxy KPI evaluation.\n",
        "    genre_sets = (\n",
        "        df[\"listed_in\"]\n",
        "        .fillna(\"\")                                                      # Replace missing genres with empty strings.\n",
        "        .str.lower()                                                     # Standardize casing to avoid duplicate tokens.\n",
        "        .str.replace(\",\", \"\", regex=False)                               # Remove commas to align with whitespace tokenization.\n",
        "        .str.split()                                                     # Split on whitespace to produce clean genre tokens.\n",
        "        .apply(set)                                                      # Convert token lists to sets for Jaccard similarity.\n",
        "    )\n",
        "\n",
        "    # Pre-group titles by content type\n",
        "    # Purpose: Reduce unnecessary O(NÂ²) comparisons and prevent invalid\n",
        "    # cross-type recommendations (e.g., Movies vs TV Shows).\n",
        "    type_to_indices = (\n",
        "        df.reset_index()\n",
        "          .groupby(\"type\")[\"index\"]\n",
        "          .apply(list)\n",
        "          .to_dict()\n",
        "    )\n",
        "\n",
        "    for content_type, indices in type_to_indices.items():               # Iterate only within the same content type.\n",
        "        for i in indices:\n",
        "            for j in indices:\n",
        "                if i == j:\n",
        "                    continue                                            # Skip self-comparisons to avoid trivial matches.\n",
        "\n",
        "                same_type = float(type_series.iloc[i] == type_series.iloc[j])  # Same-type match supports type-consistent recommendations.\n",
        "                inter = genre_sets.iloc[i] & genre_sets.iloc[j]                # Compute shared genre tokens between titles.\n",
        "                union = genre_sets.iloc[i] | genre_sets.iloc[j]                # Compute total unique genre tokens across both titles.\n",
        "                jaccard = (len(inter) / len(union)) if len(union) else 0.0     # Jaccard similarity for interpretable set overlap.\n",
        "\n",
        "                sim[i, j] = (0.6 * same_type) + (0.4 * jaccard)                # Weight type higher to avoid mismatched suggestions;\n",
        "                                                                               # keep genre overlap as secondary relevance signal.\n",
        "\n",
        "    return sim\n",
        "\n",
        "rules_sim_matrix = build_rules_based_similarity(df)                      # Compute rules-based similarity for baseline comparison.\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 4b. Model 1: TF-IDFâ€“Based Content Similarity Model (Cosine Similarity)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "# Initialize TF-IDF vectorizer\n",
        "# Purpose: Convert cleaned text content into numerical features that reflect\n",
        "# term importance across the entire Netflix catalog.\n",
        "tfidf = TfidfVectorizer(\n",
        "    stop_words=\"english\",          # Remove common English words that add little meaning.\n",
        "    max_features=5000              # Limit feature space to reduce noise and computation cost.\n",
        "                                   # 5000 instead of 1000 or 10000 was chosen to capture the most informative terms while avoiding sparse,\n",
        "                                   # high-dimensional vectors that can degrade similarity quality.\n",
        ")\n",
        "\n",
        "# Apply TF-IDF transformation\n",
        "# Purpose: Learn vocabulary from the catalog and transform each titleâ€™s\n",
        "# content profile into a numerical feature vector.\n",
        "tfidf_matrix = tfidf.fit_transform(df[\"content_profile\"])  # Learn a shared vocabulary from the entire catalog and transform each titleâ€™s\n",
        "                                                           # content profile into a TF-IDF vector, ensuring consistent feature space\n",
        "                                                           # for fair and comparable similarity calculations across titles.\n",
        "\n",
        "# Compute cosine similarity matrix\n",
        "# Purpose: Measure how similar each title is to every other title while\n",
        "# controlling memory usage for large catalogs.\n",
        "cosine_sim_matrix = cosine_similarity(\n",
        "    tfidf_matrix,\n",
        "    tfidf_matrix,\n",
        "    dense_output=False                              # Keep output sparse to reduce memory consumption for large item catalogs.\n",
        ").astype(\"float32\")                                 # Downcast precision to float32 to further reduce memory footprint\n",
        "                                                    # with negligible impact on similarity ranking.\n",
        "\n",
        "# Validate matrix dimensions\n",
        "# Purpose: Ensure TF-IDF and similarity matrices align with the number of titles.\n",
        "print(\"TF-IDF matrix shape:\", tfidf_matrix.shape)                   # Confirm number of titles and features.\n",
        "print(\"Cosine similarity matrix shape:\", cosine_sim_matrix.shape)   # Confirm item-to-item similarity structure.\n",
        "print(\n",
        "    f\"The TF-IDF matrix represents {tfidf_matrix.shape[0]} titles using {tfidf_matrix.shape[1]} weighted text features.\\n\"\n",
        "    f\"The cosine similarity matrix correctly computes pairwise similarity \\n\"\n",
        "    f\"across all {cosine_sim_matrix.shape[0]} titles, enabling full catalog recommendations.\\n\"\n",
        ")\n",
        "print(\"\\n\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 4c. Model 2: Weighted Content Similarity (Hybrid Content-Based)\n",
        "# ---------------------------------------------------------------------------\n",
        "# Purpose: Improve control and business alignment by weighting distinct\n",
        "# content signals (e.g., genre vs description) instead of treating all text\n",
        "# as equally important.\n",
        "\n",
        "tfidf_genre = TfidfVectorizer(                                        # Build a genre-specific TF-IDF representation.\n",
        "    stop_words=\"english\",                                             # Remove common terms to reduce noise in sparse vectors.\n",
        "    max_features=2000                                                 # Smaller cap because genre vocabulary is typically limited; larger\n",
        ")                                                                     # values can increase sparsity without improving signal quality.\n",
        "\n",
        "tfidf_desc = TfidfVectorizer(                                         # Build a description-specific TF-IDF representation.\n",
        "    stop_words=\"english\",                                             # Remove common terms to focus on thematic keywords.\n",
        "    max_features=3000                                                 # Moderate cap to capture richer plot language while limiting extreme\n",
        ")                                                                     # dimensionality that can degrade similarity stability.\n",
        "\n",
        "genre_matrix = tfidf_genre.fit_transform(df[\"listed_in\"])             # Vectorize genre text as a structured category signal.\n",
        "desc_matrix = tfidf_desc.fit_transform(df[\"description\"])             # Vectorize description text as a semantic theme signal.\n",
        "\n",
        "genre_sim_matrix = cosine_similarity(genre_matrix, genre_matrix)      # Similarity based on category alignment across the catalog.\n",
        "desc_sim_matrix = cosine_similarity(desc_matrix, desc_matrix)         # Similarity based on thematic alignment across descriptions.\n",
        "\n",
        "def build_weighted_similarity(genre_sim, desc_sim, w_genre=0.6):      # Combine similarity matrices using explicit weights.\n",
        "    \"\"\"\n",
        "    Build a weighted similarity matrix.\n",
        "\n",
        "    Purpose:\n",
        "    Combine genre-driven and description-driven similarity so the recommender\n",
        "    can reflect business preferences (e.g., consistent genres while still\n",
        "    surfacing thematically related titles).\n",
        "    \"\"\"\n",
        "    w_desc = 1 - w_genre                                              # Constrain weights to sum to 1 for interpretability and stability.\n",
        "    return (w_genre * genre_sim) + (w_desc * desc_sim)                # Blend signals into one similarity matrix for retrieval.\n",
        "\n",
        "weighted_sim_matrix = build_weighted_similarity(                      # Create initial Model 2 similarity matrix.\n",
        "    genre_sim_matrix,\n",
        "    desc_sim_matrix,\n",
        "    w_genre=0.6                                                       # Start with genre-weighted emphasis since genre is usually the most\n",
        ")                                                                     # interpretable and stable content signal in a catalog.\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 4d. Embeddings: Semantic Similarity (Encoder-Based Model)\n",
        "# ---------------------------------------------------------------------------\n",
        "# Purpose: Capture deeper semantic relationships beyond token overlap (TF-IDF)\n",
        "# by using a pretrained neural text encoder. This encoder maps content text\n",
        "# into dense vector representations where semantic meaning, context, and\n",
        "# paraphrasing are preserved.\n",
        "\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer               # Import a pretrained neural text encoder for sentence-level semantics.\n",
        "\n",
        "    embedding_model = SentenceTransformer(                              # Initialize a pretrained sentence encoder.\n",
        "        \"all-MiniLM-L6-v2\"                                              # Compact transformer-based encoder chosen for strong semantic\n",
        "    )                                                                   # performance with lower computational cost.\n",
        "\n",
        "    content_text = df[\"content_profile\"].fillna(\"\").tolist()            # Prepare content profiles as raw text inputs for the encoder.\n",
        "\n",
        "    embeddings = embedding_model.encode(                                # Encode each titleâ€™s content profile into a dense semantic vector\n",
        "        content_text,                                                   # using the neural encoder rather than token-based statistics.\n",
        "        show_progress_bar=True                                          # Display progress to improve transparency during batch encoding.\n",
        "    )\n",
        "\n",
        "    embed_sim_matrix = cosine_similarity(embeddings, embeddings)        # Compute cosine similarity between encoder-generated embeddings,\n",
        "                                                                        # measuring semantic closeness rather than surface-level term overlap.\n",
        "except Exception as e:\n",
        "    embed_sim_matrix = None                                             # Skip encoder-based modeling if dependencies or\n",
        "    print(\"Embeddings model skipped (dependency or runtime issue):\", e) # runtime constraints prevent encoder initialization.\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 4e. Cross-validation (Recommender-Appropriate CV Analogue)\n",
        "# ---------------------------------------------------------------------------\n",
        "# Purpose: Evaluate stability by repeating Top-K retrieval across multiple\n",
        "# query titles and aggregating proxy metrics. Traditional k-fold CV is not\n",
        "# used because there is no labeled ground truth.\n",
        "\n",
        "def proxy_precision_at_k(query_idx, rec_indices, df):                 # Proxy relevance using genre overlap (interpretable and data-available).\n",
        "    \"\"\"\n",
        "    Compute proxy Precision@K.\n",
        "\n",
        "    Purpose:\n",
        "    Approximate relevance without user labels by counting recommendations\n",
        "    that share at least one genre token with the query title.\n",
        "    \"\"\"\n",
        "    query_genres = set(str(df.loc[query_idx, \"listed_in\"]).split())    # Tokenize query genres to define a relevance proxy.\n",
        "    if not query_genres:\n",
        "        return 0.0                                                    # If no genres exist, relevance proxy cannot be computed reliably.\n",
        "\n",
        "    hits = 0                                                          # Count of \"relevant\" recommendations under the proxy rule.\n",
        "    for ridx in rec_indices:\n",
        "        rec_genres = set(str(df.loc[ridx, \"listed_in\"]).split())       # Tokenize recommended title genres.\n",
        "        hits += int(len(query_genres & rec_genres) > 0)                # Treat at least one shared genre token as a proxy hit.\n",
        "\n",
        "    return hits / len(rec_indices) if rec_indices else 0.0             # Normalize to 0â€“1 to match KPI scale.\n",
        "\n",
        "def get_top_k_indices(query_idx, sim_matrix, top_k=10):                # Retrieve Top-K indices from any similarity matrix.\n",
        "    \"\"\"\n",
        "    Retrieve Top-K indices.\n",
        "\n",
        "    Purpose:\n",
        "    Standardize retrieval across all similarity models so evaluation is\n",
        "    consistent and comparable.\n",
        "    \"\"\"\n",
        "\n",
        "    # Handle sparse vs dense similarity matrices safely\n",
        "    if hasattr(sim_matrix, \"toarray\"):                                 # Check if similarity matrix is sparse (e.g., CSR).\n",
        "        scores = sim_matrix[query_idx].toarray().ravel()               # Convert only the query row to dense for sorting.\n",
        "    else:\n",
        "        scores = sim_matrix[query_idx]                                 # Use directly if already dense.\n",
        "\n",
        "    scores = list(enumerate(scores))                                   # Pair each candidate index with its similarity score.\n",
        "    scores = sorted(scores, key=lambda x: x[1], reverse=True)          # Sort from highest similarity to lowest.\n",
        "    scores = [s for s in scores if s[0] != query_idx]                  # Remove self-match to avoid trivial recommendations.\n",
        "    return [i for i, _ in scores[:top_k]]                              # Return Top-K candidate indices.\n",
        "\n",
        "def proxy_ild(rec_indices, df):                                        # Proxy ILD using genre-set diversity among recommended items.\n",
        "    \"\"\"\n",
        "    Compute proxy Intra-list Diversity (ILD).\n",
        "\n",
        "    Purpose:\n",
        "    Estimate variety within a recommendation list using genre overlap\n",
        "    among recommended titles, where higher values indicate more variety.\n",
        "    \"\"\"\n",
        "    rec_genre_sets = (\n",
        "        df.loc[rec_indices, \"listed_in\"].fillna(\"\").str.split(\" \").apply(set)\n",
        "    )                                                                  # Convert each recommendationâ€™s genres into sets for set comparisons.\n",
        "\n",
        "    similarities = []                                                  # Store pairwise Jaccard overlaps for the recommendation list.\n",
        "    for i in range(len(rec_genre_sets)):\n",
        "        for j in range(i + 1, len(rec_genre_sets)):\n",
        "            inter = rec_genre_sets.iloc[i] & rec_genre_sets.iloc[j]    # Shared genres between two recommendations.\n",
        "            union = rec_genre_sets.iloc[i] | rec_genre_sets.iloc[j]    # Total unique genres between two recommendations.\n",
        "            similarities.append((len(inter) / len(union)) if len(union) else 0.0)\n",
        "\n",
        "    return 1 - float(np.mean(similarities)) if similarities else 0.0   # Convert overlap to diversity so higher is better.\n",
        "\n",
        "def proxy_catalog_coverage(all_rec_indices, total_titles):             # Proxy coverage based on unique titles surfaced across queries.\n",
        "    \"\"\"\n",
        "    Compute proxy Catalog Coverage (CC).\n",
        "\n",
        "    Purpose:\n",
        "    Measure how broadly recommendations surface the catalog across\n",
        "    multiple queries.\n",
        "    \"\"\"\n",
        "    return len(set(all_rec_indices)) / total_titles                    # Normalize by catalog size to keep metric on 0â€“1 scale.\n",
        "\n",
        "def evaluate_similarity_model(sim_matrix, df, query_indices, top_k=10): # Evaluate similarity model over repeated queries.\n",
        "    \"\"\"\n",
        "    Evaluate similarity model via repeated-query aggregation.\n",
        "\n",
        "    Purpose:\n",
        "    Provide a CV-like robustness check by evaluating proxy KPIs across\n",
        "    multiple query titles rather than a single example.\n",
        "    \"\"\"\n",
        "    p_at_k_scores = []                                                 # Store proxy precision for each query title.\n",
        "    ild_scores = []                                                    # Store diversity for each query title.\n",
        "    all_recs = []                                                      # Aggregate recommendations across queries for coverage.\n",
        "\n",
        "    for qidx in query_indices:\n",
        "        recs = get_top_k_indices(qidx, sim_matrix, top_k=top_k)        # Retrieve Top-K recommendations for the query.\n",
        "        all_recs.extend(recs)                                          # Collect all recommendation indices for coverage computation.\n",
        "        p_at_k_scores.append(proxy_precision_at_k(qidx, recs, df))     # Compute proxy relevance.\n",
        "        ild_scores.append(proxy_ild(recs, df))                         # Compute list diversity.\n",
        "\n",
        "    return {\n",
        "        \"Precision@K (proxy)\": float(np.mean(p_at_k_scores)),          # Average proxy relevance across query titles.\n",
        "        \"ILD\": float(np.mean(ild_scores)),                             # Average diversity across query titles.\n",
        "        \"Catalog Coverage\": float(proxy_catalog_coverage(all_recs, df.shape[0]))  # Coverage across all queries.\n",
        "    }\n",
        "\n",
        "query_indices = list(range(min(50, df.shape[0])))                      # Use first 50 titles as a stable evaluation subset to keep\n",
        "                                                                       # runtime manageable while still covering varied catalog entries.\n",
        "\n",
        "baseline_metrics = evaluate_similarity_model(                          # Evaluate baseline rules-based similarity.\n",
        "    rules_sim_matrix, df, query_indices, top_k=10                      # top_k=10 aligns with Step 5 default for consistency.\n",
        ")\n",
        "\n",
        "model_1_metrics = evaluate_similarity_model(                           # Evaluate Model 1 (TF-IDF content_profile).\n",
        "    cosine_sim_matrix, df, query_indices, top_k=10\n",
        ")\n",
        "\n",
        "model_2_metrics = evaluate_similarity_model(                           # Evaluate Model 2 (weighted hybrid, initial weights).\n",
        "    weighted_sim_matrix, df, query_indices, top_k=10\n",
        ")\n",
        "\n",
        "if embed_sim_matrix is not None:\n",
        "    embed_metrics = evaluate_similarity_model(                         # Evaluate embeddings similarity model if available.\n",
        "        embed_sim_matrix, df, query_indices, top_k=10\n",
        "    )\n",
        "else:\n",
        "    embed_metrics = None\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 4f. Model 2 - Hyperparameter Tuned (Weight Search)\n",
        "# ---------------------------------------------------------------------------\n",
        "# Purpose: Tune Model 2 by testing multiple genre weights and selecting the\n",
        "# configuration that best balances relevance and diversity.\n",
        "\n",
        "weight_grid = [0.4, 0.5, 0.6, 0.7, 0.8]                                # Test a compact grid to balance thoroughness and runtime; grid spans\n",
        "                                                                       # reasonable trade-offs between genre (structure) and description (theme).\n",
        "\n",
        "tuning_rows = []                                                       # Store metrics per candidate weight.\n",
        "for w in weight_grid:\n",
        "    sim = build_weighted_similarity(genre_sim_matrix, desc_sim_matrix, w_genre=w) # Build weighted similarity for candidate weight.\n",
        "    metrics = evaluate_similarity_model(sim, df, query_indices, top_k=10)         # Evaluate consistently across the same query set.\n",
        "\n",
        "    tuning_rows.append({\n",
        "        \"Model\": \"Model 2 (Tuned Candidate)\",\n",
        "        \"Genre Weight\": w,\n",
        "        **metrics\n",
        "    })\n",
        "\n",
        "tuning_df = pd.DataFrame(tuning_rows)                                  # Convert tuning results to a table for comparison.\n",
        "\n",
        "tuning_df[\"Selection Score\"] = (                                       # Use a simple composite score to select the best configuration; weights\n",
        "    0.50 * tuning_df[\"Precision@K (proxy)\"] +                          # emphasize relevance while still rewarding diversity and exposure.\n",
        "    0.25 * tuning_df[\"ILD\"] +\n",
        "    0.25 * tuning_df[\"Catalog Coverage\"]\n",
        ")\n",
        "\n",
        "best_tuned_row = tuning_df.sort_values(\"Selection Score\", ascending=False).iloc[0] # Pick the highest-scoring configuration.\n",
        "best_genre_weight = float(best_tuned_row[\"Genre Weight\"])                               # Extract best genre weight for final tuned model.\n",
        "\n",
        "tuned_weighted_sim_matrix = build_weighted_similarity(                  # Build final tuned Model 2 similarity matrix.\n",
        "    genre_sim_matrix,\n",
        "    desc_sim_matrix,\n",
        "    w_genre=best_genre_weight\n",
        ")\n",
        "\n",
        "model_2_tuned_metrics = evaluate_similarity_model(                      # Evaluate tuned Model 2 for reporting against all other models.\n",
        "    tuned_weighted_sim_matrix, df, query_indices, top_k=10\n",
        ")\n",
        "print(\"\\n\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 4g. Produce a Model Comparison Summary of Performance Between All Models\n",
        "# ---------------------------------------------------------------------------\n",
        "# Purpose: Consolidate proxy KPI performance into a single comparison table\n",
        "# for rubric-aligned reporting and decision-making.\n",
        "\n",
        "# Consolidate model evaluation results into a single summary table\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "summary_rows = [\n",
        "    {\"Model\": \"Baseline â€“ Rules Based\", **baseline_metrics},\n",
        "    {\"Model\": \"Model 1 â€“ TF-IDF (Current)\", **model_1_metrics},\n",
        "    {\"Model\": \"Model 2 â€“ Weighted (Initial)\", **model_2_metrics},\n",
        "    {\"Model\": \"Model 2 â€“ Weighted (Tuned)\", **model_2_tuned_metrics},\n",
        "]\n",
        "\n",
        "if embed_metrics is not None:\n",
        "    summary_rows.append(\n",
        "        {\"Model\": \"Embeddings â€“ Semantic Similarity\", **embed_metrics}\n",
        "    )\n",
        "\n",
        "performance_summary = pd.DataFrame(summary_rows)\n",
        "\n",
        "\n",
        "model_comparison_table = performance_summary.copy()   # Preserve original metrics table\n",
        "\n",
        "# Reorder and rename columns for presentation clarity\n",
        "model_comparison_table = model_comparison_table[[\n",
        "    \"Model\",\n",
        "    \"Precision@K (proxy)\",\n",
        "    \"ILD\",\n",
        "    \"Catalog Coverage\"\n",
        "]]\n",
        "\n",
        "model_comparison_table.columns = [\n",
        "    \"Model\",\n",
        "    \"Precision@K\",\n",
        "    \"Intra-list Diversity (ILD)\",\n",
        "    \"Catalog Coverage\"\n",
        "]\n",
        "\n",
        "display(model_comparison_table)\n",
        "print(\"\\n\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 4h. Visualizations â€“ Tuning Curve and Model Comparison\n",
        "# ---------------------------------------------------------------------------\n",
        "# Purpose: Visualize the tuning trade-offs and compare model performance\n",
        "# across KPIs for clear communication.\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(tuning_df[\"Genre Weight\"], tuning_df[\"Precision@K (proxy)\"], marker=\"o\", label=\"Precision@K (proxy)\")\n",
        "plt.plot(tuning_df[\"Genre Weight\"], tuning_df[\"ILD\"], marker=\"o\", label=\"ILD\")\n",
        "plt.plot(tuning_df[\"Genre Weight\"], tuning_df[\"Catalog Coverage\"], marker=\"o\", label=\"Catalog Coverage\")\n",
        "plt.title(\"Model 2 Hyperparameter Tuning â€“ Metric Trends vs Genre Weight\")\n",
        "plt.xlabel(\"Genre Weight\")\n",
        "plt.ylabel(\"Metric Value (0â€“1)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print(\n",
        "    f\"INTERPRETATION:\\n Precision@K peaks around a genre weight of {tuning_df.loc[tuning_df['Precision@K (proxy)'].idxmax(), 'Genre Weight']}, \\n\"\n",
        "    f\" after which gains in relevance diminish while diversity and catalog coverage \\n \"\n",
        "    f\"begin to decline. This highlights a trade-off where excessive genre weighting \\n \"\n",
        "    f\"leads to narrower recommendations, validating the need for balanced weighting. \\n\"\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "x = np.arange(len(performance_summary[\"Model\"]))\n",
        "plt.bar(x - 0.25, performance_summary[\"Precision@K (proxy)\"], width=0.25, label=\"Precision@K (proxy)\")\n",
        "plt.bar(x,         performance_summary[\"ILD\"],                 width=0.25, label=\"ILD\")\n",
        "plt.bar(x + 0.25, performance_summary[\"Catalog Coverage\"],    width=0.25, label=\"Catalog Coverage\")\n",
        "plt.xticks(x, performance_summary[\"Model\"], rotation=45, ha=\"right\")\n",
        "plt.title(\"Performance Comparison Across Models (Proxy KPIs)\")\n",
        "plt.ylabel(\"Metric Value (0â€“1)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"\\n\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 4i. Perform Final Selection\n",
        "# ---------------------------------------------------------------------------\n",
        "# Purpose: Select the single similarity matrix that best balances proxy\n",
        "# KPIs for downstream use in Step 5.\n",
        "\n",
        "# Compute composite selection score\n",
        "# Purpose: Align final model selection directly with Step 1 success\n",
        "# criteria by prioritizing relevance while enforcing diversity and\n",
        "# catalog exposure.\n",
        "performance_summary[\"Selection Score\"] = (\n",
        "    0.50 * performance_summary[\"Precision@K (proxy)\"] +   # Primary KPI: relevance.\n",
        "    0.30 * performance_summary[\"ILD\"] +                   # Secondary KPI: recommendation diversity.\n",
        "    0.20 * performance_summary[\"Catalog Coverage\"]        # Tertiary KPI: fair catalog exposure.\n",
        ")\n",
        "\n",
        "final_choice = performance_summary.sort_values(\"Selection Score\", ascending=False).iloc[0]  # Identify the top-performing model.\n",
        "\n",
        "# Display Selection Scores Across All Evaluated Models\n",
        "# ---------------------------------------------------------------------------\n",
        "# Purpose: Provide full transparency into how each model performed under the\n",
        "# composite selection criteria before choosing the final recommender.\n",
        "\n",
        "selection_score_table = (\n",
        "    performance_summary[\n",
        "        [\"Model\", \"Precision@K (proxy)\", \"ILD\", \"Catalog Coverage\", \"Selection Score\"]\n",
        "    ]\n",
        "    .sort_values(\"Selection Score\", ascending=False)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "print(\"Model Selection Score Comparison (All Candidates)\")\n",
        "display(selection_score_table)\n",
        "final_model_name = str(final_choice[\"Model\"])                                                # Store selected model name for reporting.\n",
        "best_model = final_model_name\n",
        "best_row = final_choice\n",
        "\n",
        "print(\n",
        "    f\"\"\"\n",
        "INTERPRETATION:\\n\n",
        "Across all evaluated approaches, the {best_model} achieved the strongest\n",
        "overall balance between relevance, diversity, and catalog exposure.\n",
        "\n",
        "It recorded a Precision@K of {best_row['Precision@K (proxy)']:.2f},\n",
        "indicating more consistent relevance compared to simpler baselines,\n",
        "while maintaining an Intra-list Diversity score of {best_row['ILD']:.2f},\n",
        "which helps avoid repetitive recommendations.\n",
        "\n",
        "Catalog Coverage reached {best_row['Catalog Coverage']:.2f}, confirming\n",
        "that this model surfaces a broader portion of the catalog rather than\n",
        "repeatedly promoting the same titles. These results justify selecting\n",
        "this model as the final recommender for downstream recommendation delivery.\n",
        "\"\"\"\n",
        ")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Map the selected model name to its corresponding similarity matrix\n",
        "# Purpose: Provide a single similarity matrix variable for Step 5 to consume.\n",
        "if final_model_name == \"Baseline â€“ Rules Based\":\n",
        "    selected_similarity_matrix = rules_sim_matrix\n",
        "elif final_model_name == \"Model 1 â€“ TF-IDF (Current)\":\n",
        "    selected_similarity_matrix = cosine_sim_matrix\n",
        "elif final_model_name == \"Model 2 â€“ Weighted (Initial)\":\n",
        "    selected_similarity_matrix = weighted_sim_matrix\n",
        "elif final_model_name == \"Model 2 â€“ Weighted (Tuned)\":\n",
        "    selected_similarity_matrix = tuned_weighted_sim_matrix\n",
        "elif final_model_name == \"Embeddings â€“ Semantic Similarity\":\n",
        "    selected_similarity_matrix = embed_sim_matrix\n",
        "else:\n",
        "    selected_similarity_matrix = cosine_sim_matrix                      # Safe fallback to Model 1 to avoid runtime failure.\n",
        "\n",
        "print(\"\\nFinal Model Selected:\", final_model_name)                      # Display which model was chosen.\n",
        "print(final_choice.to_string())                                         # Display the chosen modelâ€™s KPI results.\n",
        "print(\"\\n\")\n",
        "print(\n",
        "    f\"The final selection of '{final_model_name}' reflects a deliberate \\n\"\n",
        "    f\"trade-off between relevance, diversity, and catalog exposure, consistent with \\n\"\n",
        "    f\"the success criteria defined during problem framing. This ensures that the chosen \\n\"\n",
        "    f\"model is not only accurate but also robust, explainable, and suitable for \\n\"\n",
        "    f\"real-world deployment under data-limited conditions. \\n\"\n",
        ")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Persist Final Recommender Artifacts (Reproducibility)\n",
        "# ---------------------------------------------------------------------------\n",
        "# Purpose: Save the final selected similarity artifacts to support\n",
        "# reproducibility and downstream reuse without retraining.\n",
        "\n",
        "models_path = \"models/final_recommender_artifacts.pkl\"\n",
        "os.makedirs(os.path.dirname(models_path), exist_ok=True)\n",
        "\n",
        "final_recommender_artifacts = {\n",
        "    \"final_model_name\": final_model_name,\n",
        "    \"genre_weight\": best_genre_weight if \"best_genre_weight\" in globals() else None,\n",
        "    \"description_weight\": (\n",
        "        1 - best_genre_weight if \"best_genre_weight\" in globals() else None\n",
        "    ),\n",
        "    \"similarity_matrix\": selected_similarity_matrix\n",
        "}\n",
        "\n",
        "with open(models_path, \"wb\") as f:\n",
        "    pickle.dump(final_recommender_artifacts, f)\n",
        "\n",
        "print(f\"Final recommender artifacts saved to {models_path}\")\n",
        "\n",
        "# Backward compatibility for Step 5\n",
        "# Purpose: Allow Step 5 to run without code changes by reusing the expected\n",
        "# similarity matrix variable name.\n",
        "cosine_sim_matrix = selected_similarity_matrix                          # Step 5 will use this matrix for recommendations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ð“‚ƒðŸ–Š Key Findings\n",
        "\n",
        "Models were evaluated progressively, starting with a rules-based baseline for full explainability, then TF-IDF to capture term relevance, followed by a weighted hybrid model to explicitly control the relevanceâ€“diversity trade-off, and finally semantic embeddings to capture deeper contextual similarity. Rules-based and heavily weighted TF-IDF models achieve perfect Precision@K but show very low Intra-list Diversity, indicating repetitive recommendations. Standard TF-IDF improves diversity but weakens relevance stability, while tuning confirms that excessive genre weighting narrows recommendations. Hyperparameter tuning was applied only to Model 2 because it uniquely exposes an interpretable control parameter; other models are fixed or pretrained. Overall, the embedding-based model provides the best balance, achieving high relevance (Precision@K = 0.96), stronger diversity (ILD = 0.34), and stable catalog exposure.\n",
        "\n",
        "\n",
        "*   **Vectorization Scale**:\n",
        "    *  TF-IDF matrix: **8,809 titles Ã— 5,000 features**\n",
        "    *  Cosine similarity: **8,809 Ã— 8,809** (full-catalog retrieval enabled)\n",
        "\n",
        "*   **Model Comparison (Proxy KPIs)**:\n",
        "    *  **Baseline (Rules-based)**: Precision@K **1.00** ILD **0.03**, Coverage **0.05**\n",
        "    *  **TF-IDF (Model 1)**: Precision@K **0.82**, ILD **0.55**, Coverage **0.05**\n",
        "    *  **Weighted TF-IDF (Tuned)**: Precision@K **1.00**, ILD **0.19**, Coverage **0.05**\n",
        "    *  **Embeddings (Semantic)**: Precision@K **0.96**, ILD **0.34**, Coverage **0.05**\n",
        "\n",
        "*   Hyperparameter Tuning (Model 2):\n",
        "    *  Genre weight tested: **0.4â€“0.8**\n",
        "    *  Precision@K peaks around **0.4**; higher weights reduce diversity and coverage\n",
        "    *  Confirms relevanceâ€“diversity trade-off\n",
        "\n",
        "*   Final Model Selected: Embeddings â€“ Semantic Similarity\n",
        "    *  Precision@K: **0.96**\n",
        "    *  Intra-list Diversity: **0.34**\n",
        "    *  Catalog Coverage: **0.05**\n",
        "    *  Composite Selection Score: **0.59**\n",
        "\n",
        "*   Business & Technical Rationale:\n",
        "    *  Strong relevance without excessive repetition\n",
        "    *  Improved semantic matching beyond keyword overlap\n",
        "    *  Maintains diversity and catalog exposure\n",
        "    *  Explainable and robust for real-world, data-limited deployment\n",
        "\n",
        "*   Output:\n",
        "    *  Final similarity artifacts saved for downstream recommendation delivery\n",
        "---"
      ],
      "metadata": {
        "id": "4XSHvKrNc5n7"
      }
    }
  ]
}