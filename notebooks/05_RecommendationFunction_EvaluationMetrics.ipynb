{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ’¡ Step 5: Recommendation Function & Evaluation Metrics\n",
        "\n",
        "---\n",
        "This step operationalizes the final selected similarity model by generating Top-K recommendations and evaluating their quality using list-level proxy metrics appropriate for content-based systems under data-limited conditions.\n",
        "\n",
        "*   **Approach**: Item-to-item Top-K retrieval using the final similarity matrix\n",
        "*   **Evaluation**: List-level proxy metrics (ILD, Catalog Coverage)\n",
        "*   **Scope**: Multiple anchor titles to assess robustness and consistency\n",
        "*   **Constraint**: No user behavior, ratings, or supervised labels\n"
      ],
      "metadata": {
        "id": "acV685kMLC2U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3fyaqimLCaU"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# STEP 5: RECOMMENDATION FUNCTION & EVALUATION METRICS\n",
        "# =============================================================================\n",
        "\n",
        "\"\"\"\n",
        "Generate Top-K content recommendations and evaluate their quality using\n",
        "proxy metrics appropriate for content-based recommendation systems.\n",
        "\n",
        "This step uses the final similarity matrix selected during model evaluation and selection,\n",
        "which represents the best-performing content similarity approach among the tested alternatives.\n",
        "This matrix is applied consistently to generate recommendations and assess their quality\n",
        "under data-limited conditions.\n",
        "\n",
        "What this step does:\n",
        "- Defines a reusable recommendation function\n",
        "- Retrieves Top-K similar titles for a given item\n",
        "- Computes proxy evaluation metrics aligned with Step 1 KPIs\n",
        "\n",
        "What this step does NOT do:\n",
        "- Train a predictive model\n",
        "- Use user behavior or ratings\n",
        "- Perform offline A/B testing\n",
        "\"\"\"\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 5a. Recommendation Retrieval Function\n",
        "# ---------------------------------------------------------------------------\n",
        "# Purpose: Retrieve Top-K similar titles based on the *final selected*\n",
        "# content similarity matrix produced in Step 4, ensuring continuity\n",
        "# between model selection and recommendation generation.\n",
        "\n",
        "def get_recommendations(title, df, similarity_matrix, top_k=10):      # Retrieve Top-K similar titles.\n",
        "                                                                      # top_k is set to 10 to align with the evaluation\n",
        "                                                                      # and selection process in Step 4 and to reflect\n",
        "                                                                      # common industry defaults for content discovery.\n",
        "    \"\"\"\n",
        "    Retrieve Top-K content recommendations for a given title.\n",
        "\n",
        "    Purpose:\n",
        "    Provide a simple and explainable way to surface similar titles based\n",
        "    on the final, selected content similarity model.\n",
        "    \"\"\"\n",
        "    if title not in df[\"title\"].values:                                # Validate that the input title exists in the catalog.\n",
        "        raise ValueError(\"Title not found in the catalog.\")\n",
        "\n",
        "    # Resolve potential duplicate title ambiguity\n",
        "    # Purpose: Ensure recommendation lookup is deterministic and does not\n",
        "    # silently select an arbitrary title when duplicates exist.\n",
        "    matches = df[df[\"title\"] == title]\n",
        "\n",
        "    if len(matches) > 1:\n",
        "        raise ValueError(\n",
        "            f\"Ambiguous title '{title}'. Multiple catalog entries found. \"\n",
        "            \"Refine the title selection or use a unique identifier.\"\n",
        "        )\n",
        "\n",
        "    idx = matches.index[0]                                             # Safe lookup after enforcing title uniqueness.\n",
        "\n",
        "    # Handle sparse vs dense similarity matrices safely\n",
        "    if hasattr(similarity_matrix, \"toarray\"):                              # Check if similarity matrix is sparse.\n",
        "        scores = similarity_matrix[idx].toarray().ravel()                  # Convert query row to dense for ranking.\n",
        "    else:\n",
        "        scores = similarity_matrix[idx]                                    # Use directly if matrix is dense.\n",
        "\n",
        "    similarity_scores = list(enumerate(scores))                            # Pair each catalog index with its similarity score.\n",
        "\n",
        "\n",
        "    # Rank titles by similarity score in descending order\n",
        "    similarity_scores = sorted(\n",
        "        similarity_scores, key=lambda x: x[1], reverse=True\n",
        "    )\n",
        "\n",
        "    # Exclude the title itself and retain Top-K results\n",
        "    similarity_scores = similarity_scores[1:top_k + 1]\n",
        "    recommended_indices = [i[0] for i in similarity_scores]            # Extract indices of recommended titles.\n",
        "\n",
        "    return df.iloc[recommended_indices][[\"title\", \"type\", \"listed_in\"]]  # Return interpretable recommendation fields.\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 5b. Intra-list Diversity (ILD)\n",
        "# ---------------------------------------------------------------------------\n",
        "# Purpose: Measure how diverse a single recommendation list is in terms\n",
        "# of genre composition, reinforcing the diversity objectives evaluated\n",
        "# during model comparison in Step 4.\n",
        "\n",
        "def compute_intra_list_diversity(recommendations):                    # Define a function to calculate recommendation diversity.\n",
        "    \"\"\"\n",
        "    Compute Intra-list Diversity (ILD) based on genre overlap.\n",
        "\n",
        "    Purpose:\n",
        "    Quantify how varied the recommended titles are to avoid repeatedly\n",
        "    suggesting near-duplicate content.\n",
        "    \"\"\"\n",
        "    genres = recommendations[\"listed_in\"].str.split(\", \")              # Split genre strings into individual genre labels.\n",
        "    genre_sets = genres.apply(set)                                     # Convert genre lists into sets for comparison.\n",
        "\n",
        "    similarities = []                                                  # Initialize storage for pairwise similarity scores.\n",
        "\n",
        "    for i in range(len(genre_sets)):\n",
        "        for j in range(i + 1, len(genre_sets)):\n",
        "            intersection = genre_sets.iloc[i] & genre_sets.iloc[j]     # Identify shared genres between two titles.\n",
        "            union = genre_sets.iloc[i] | genre_sets.iloc[j]            # Identify total unique genres.\n",
        "            similarities.append(len(intersection) / len(union))        # Jaccard similarity for categorical overlap.\n",
        "\n",
        "    if not similarities:                                               # Handle edge cases with insufficient comparisons.\n",
        "        return 0.0\n",
        "\n",
        "    return 1 - np.mean(similarities)                                   # Higher values indicate greater diversity.\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 5c. Catalog Coverage\n",
        "# ---------------------------------------------------------------------------\n",
        "# Purpose: Measure how much of the catalog is exposed by a single\n",
        "# recommendation list, complementing the multi-query coverage analysis\n",
        "# performed during Step 4 evaluation.\n",
        "\n",
        "def compute_catalog_coverage(recommended_titles, total_titles):       # Define a function to compute catalog exposure.\n",
        "    \"\"\"\n",
        "    Compute Catalog Coverage (CC).\n",
        "\n",
        "    Purpose:\n",
        "    Assess whether recommendations surface a broad range of titles\n",
        "    rather than repeatedly promoting the same items.\n",
        "    \"\"\"\n",
        "    unique_recommended = recommended_titles[\"title\"].nunique()         # Count unique titles in the recommendation list.\n",
        "    return unique_recommended / total_titles                           # Normalize by total catalog size.\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 5d. Example Recommendation & Metric Inspection\n",
        "# ---------------------------------------------------------------------------\n",
        "# Purpose: Demonstrate how the selected model behaves for an individual\n",
        "# title and how proxy metrics can be interpreted at the recommendation\n",
        "# list level.\n",
        "\n",
        "example_titles = [\n",
        "    \"Dick Johnson Is Dead\",        # Example Title #1 (fixed anchor)\n",
        "    df[\"title\"].iloc[6847],        # Example Title #2, for this and the remaining titles, the index locations are randomly picked\n",
        "    df[\"title\"].iloc[94],          # Example Title #3\n",
        "    df[\"title\"].iloc[198],         # Example Title #4\n",
        "    df[\"title\"].iloc[4156]         # Example Title #5\n",
        "]\n",
        "\n",
        "\n",
        "for idx, example_title in enumerate(example_titles, start=1):\n",
        "\n",
        "    recommendations = get_recommendations(     # Generate Top-K recommendations using the\n",
        "        example_title,                          # final similarity matrix selected in Step 4.\n",
        "        df,\n",
        "        cosine_sim_matrix,                       # Backward-compatible variable name mapped\n",
        "        top_k=10                                # to the selected model.\n",
        "    )\n",
        "\n",
        "    ild_score = compute_intra_list_diversity(recommendations)     # Compute Intra-list Diversity score.\n",
        "    catalog_coverage = compute_catalog_coverage(                  # Compute Catalog Coverage score.\n",
        "        recommendations, df.shape[0]\n",
        "    )\n",
        "\n",
        "    print(f\"EXAMPLE TITLE #{idx}: {example_title}\")                                       # Display example title\n",
        "    print(\"\\n\")\n",
        "\n",
        "    example_description = df.loc[df[\"title\"] == example_title, \"description\"].values[0]   # Display description for contextual grounding\n",
        "\n",
        "    print(\"DESCRIPTION:\")\n",
        "    print(fill(example_description, width=100))                                           # Wrap text for readability\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print(\n",
        "        f\"'{example_title}' serves as the anchor item \\n\"\n",
        "        f\"for evaluating how effectively the final similarity model retrieves \\n\"\n",
        "        f\"related content based on learned content similarity signals. \\n\"\n",
        "    )\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print(f\"THE TOP-{len(recommendations)} RECOMMENDATIONS\")\n",
        "    display(recommendations)                                              # Display recommended titles.\n",
        "    print(\n",
        "        f\"The Top-{len(recommendations)} recommendations reflect titles \\n\"\n",
        "        f\"that the final model considers most similar to '{example_title}'. \\n\"\n",
        "        f\"The mix of content types and genres provides an initial qualitative check \\n\"\n",
        "        f\"on relevance and thematic consistency. \\n\"\n",
        "    )\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print(\"Intra-list Diversity (ILD):\", round(ild_score, 3))             # Display diversity metric.\n",
        "    print(\n",
        "        f\"An ILD score of {ild_score:.3f} indicates the degree of variety \\n\"\n",
        "        f\"within the recommendation list. Higher values suggest reduced redundancy \\n\"\n",
        "        f\"and stronger genre diversity, aligning with the diversity targets defined \\n\"\n",
        "        f\"in the success metrics. \\n\"\n",
        "    )\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print(f\"Catalog Coverage (CC): {catalog_coverage:.3f}\")               # Display coverage metric.\n",
        "    print(\n",
        "        f\"A Catalog Coverage score of {catalog_coverage:.3f} indicates that this \\n\"\n",
        "        f\"single Top-{len(recommendations)} recommendation list surfaces roughly \\n\"\n",
        "        f\"{catalog_coverage:.1%} of the total catalog. This low value is expected \\n\"\n",
        "        f\"in a single-anchor evaluation and reflects a precision-focused retrieval \\n\"\n",
        "        f\"rather than broad catalog exploration.\\n\"\n",
        "    )\n",
        "\n",
        "    print(\"-\" * 38)\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Summary Across Example Titles\n",
        "# ---------------------------------------------------------------------------\n",
        "# Purpose: Aggregate proxy metrics across multiple anchor titles to\n",
        "# demonstrate robustness and variability in recommendation behavior.\n",
        "\n",
        "summary_rows = []\n",
        "\n",
        "for example_title in example_titles:\n",
        "    recommendations = get_recommendations(\n",
        "        example_title,\n",
        "        df,\n",
        "        cosine_sim_matrix,\n",
        "        top_k=10\n",
        "    )\n",
        "\n",
        "    summary_rows.append({\n",
        "        \"Example Title\": example_title,\n",
        "        \"Intra-list Diversity (ILD)\": round(compute_intra_list_diversity(recommendations), 3),\n",
        "        \"Catalog Coverage (CC)\": round(\n",
        "            compute_catalog_coverage(recommendations, df.shape[0]), 3\n",
        "        )\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_rows)\n",
        "\n",
        "print(\"SUMMARY OF RECOMMENDATION METRICS ACROSS EXAMPLE TITLES\")\n",
        "display(summary_df)\n",
        "print(\"\\n\")\n",
        "print(\n",
        "    \"The summary table shows that both Intra-list Diversity (ILD) and Catalog Coverage (CC) \\n\"\n",
        "    \"vary across different anchor titles. This variation is expected in a content-based \\n\"\n",
        "    \"recommender system, as titles differ in genre breadth, thematic specificity, and \\n\"\n",
        "    \"metadata richness. Narrow or niche titles tend to produce more tightly clustered \\n\"\n",
        "    \"recommendations with lower diversity, while broadly categorized titles surface \\n\"\n",
        "    \"a wider range of related content, increasing ILD and catalog exposure.\\n\\n\"\n",
        "    \"Evaluating recommendations across multiple anchor titles strengthens validation \\n\"\n",
        "    \"by demonstrating that system performance is not dependent on a single example. \\n\"\n",
        "    \"This multi-anchor analysis provides evidence of stability, robustness, and \\n\"\n",
        "    \"generalizability under data-limited conditions. By showing consistent yet \\n\"\n",
        "    \"context-sensitive behavior across diverse titles, the recommender system \\n\"\n",
        "    \"meets evaluation best practices for unsupervised, content-based models.\\n\"\n",
        ")\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# Variance Statistics (Across Example Titles)\n",
        "# ---------------------------------------------------------------------------\n",
        "# Purpose: Quantify how much recommendation diversity and exposure\n",
        "# vary across different anchor titles.\n",
        "\n",
        "ild_variance = summary_df[\"Intra-list Diversity (ILD)\"].var()\n",
        "cc_variance = summary_df[\"Catalog Coverage (CC)\"].var()\n",
        "\n",
        "ild_std = summary_df[\"Intra-list Diversity (ILD)\"].std()\n",
        "cc_std = summary_df[\"Catalog Coverage (CC)\"].std()\n",
        "\n",
        "variance_df = pd.DataFrame({\n",
        "    \"Metric\": [\"Intra-list Diversity (ILD)\", \"Catalog Coverage (CC)\"],\n",
        "    \"Variance\": [round(ild_variance, 4), round(cc_variance, 6)],\n",
        "    \"Standard Deviation\": [round(ild_std, 4), round(cc_std, 6)]\n",
        "})\n",
        "\n",
        "print(\"VARIANCE STATISTICS ACROSS EXAMPLE TITLES\")\n",
        "display(variance_df)\n",
        "print(\"\\n\")\n",
        "ild_var = variance_df.loc[variance_df[\"Metric\"] == \"Intra-list Diversity (ILD)\", \"Variance\"].values[0]\n",
        "ild_std = variance_df.loc[variance_df[\"Metric\"] == \"Intra-list Diversity (ILD)\", \"Standard Deviation\"].values[0]\n",
        "\n",
        "cc_var = variance_df.loc[variance_df[\"Metric\"] == \"Catalog Coverage (CC)\", \"Variance\"].values[0]\n",
        "cc_std = variance_df.loc[variance_df[\"Metric\"] == \"Catalog Coverage (CC)\", \"Standard Deviation\"].values[0]\n",
        "\n",
        "print(\n",
        "    f\"INTERPRETATION:\\n\"\n",
        "    f\"The Intra-list Diversity (ILD) shows low variance ({ild_var:.3f}) and a small \\n\"\n",
        "    f\"standard deviation ({ild_std:.3f}), indicating that the level of diversity \\n\"\n",
        "    f\"remains consistent across different anchor titles. This suggests stable \\n\"\n",
        "    f\"list-level behavior rather than sensitivity to any single example.\\n\\n\"\n",
        "    f\"Catalog Coverage (CC) exhibits near-zero variance ({cc_var:.3f}) and standard \\n\"\n",
        "    f\"deviation ({cc_std:.3f}), which is expected in a single-anchor, Top-K evaluation \\n\"\n",
        "    f\"setting. This confirms that coverage is structurally constrained by the fixed \\n\"\n",
        "    f\"recommendation list size rather than influenced by model instability.\\n\\n\"\n",
        "    f\"Overall, the low dispersion observed across both metrics indicates controlled \\n\"\n",
        "    f\"and predictable recommendation behavior, supporting the conclusion that the \\n\"\n",
        "    f\"model is neither overfitted nor erratic, but instead behaves consistently under \\n\"\n",
        "    f\"data-limited conditions.\\n\"\n",
        ")\n",
        "print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ð“‚ƒðŸ–Š Key Findings\n",
        "\n",
        "The evaluation shows that the final recommender produces thematically relevant and context-aware recommendations across diverse anchor titles. Intra-list Diversity adapts appropriately to content specificity, ranging from 0.20 for narrow family titles to 0.78 for broader crime and drama content, while documentaries exhibit moderate diversity (ILD 0.38â€“0.71). Catalog Coverage remains consistently low at 0.001 (â‰ˆ 0.1% per Top-10 list), reflecting an expected precision-focused retrieval strategy. Low metric variance (ILD variance 0.056; CC â‰ˆ 0.000) confirms stable and predictable behavior across multiple anchors, supporting the modelâ€™s robustness and suitability for deployment under data-limited conditions.\n",
        "\n",
        "*   **Qualitative Relevance**:\n",
        "    *  Example titles (e.g., Dick Johnson Is Dead, Ghost Rider, Show Dogs) retrieve genre- and theme-consistent recommendations, validating semantic similarity behavior\n",
        "\n",
        "*   **Intra-list Diversity (ILD)**:\n",
        "    *  Ranges from **0.20** (Show Dogs, niche family genre) to **0.78** (King of Boys: The Return of the King, broader crime/drama mix)\n",
        "    *  Documentary and politically themed titles show moderate diversity (ILD **0.38â€“0.71**)\n",
        "    *  **Variance: 0.056, Std. Dev.: 0.237**, indicating stable diversity behavior across anchors\n",
        "\n",
        "*   **Catalog Coverage (CC)**:\n",
        "    *  Consistently **0.001** per Top-10 list (â‰ˆ **0.1%** of the 8,809-title catalog)\n",
        "    *  Near-zero variance confirms coverage is structurally constrained by fixed Top-K size, not model instability\n",
        "\n",
        "*   **Robustness Validation**:\n",
        "    *  Multi-anchor evaluation shows performance is not driven by a single example\n",
        "    *  Behavior adapts to title specificity while remaining predictable and controlled\n",
        "\n",
        "*   **Business Interpretation**:\n",
        "    *  Delivers precision-focused recommendations with appropriate diversity\n",
        "    *  Avoids erratic or overly repetitive outputs\n",
        "    *  Suitable for real-world deployment in cold-start and metadata-only settings\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "nEUty50DdBnk"
      }
    }
  ]
}