{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ü§ù Step 6: Critical Thinking, Explainability, Ethical AI, and Bias Auditing\n",
        "\n",
        "---\n",
        "This step evaluates the recommender system beyond accuracy by examining explainability, ethical considerations, and bias risks. Because the solution is a content-based recommender with no user profiles or prediction labels, traditional supervised explainability and fairness techniques are not directly applicable. Instead, this step applies recommender-appropriate methods to explain why recommendations are generated, audit content exposure bias, document mitigation strategies, and transparently disclose system limitations. The goal is to ensure the system is interpretable, auditable, and responsible under real-world, data-limited conditions."
      ],
      "metadata": {
        "id": "KgiYLmV4LLqZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NzIedacLKvw"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# STEP 6: CRITICAL THINKING, EXPLAINABILITY, ETHICAL AI, AND BIAS AUDITING\n",
        "# =============================================================================\n",
        "\n",
        "\"\"\"\n",
        "Review the recommender system from an explainability, ethics, and bias\n",
        "perspective to ensure the solution is responsible, transparent, and\n",
        "aligned with real-world constraints.\n",
        "\n",
        "Because this project uses a content-based recommender without user data\n",
        "or prediction labels, explainability and fairness are handled differently\n",
        "from traditional supervised machine learning models.\n",
        "\n",
        "This step focuses on:\n",
        "- Explaining why recommendations are generated\n",
        "- Checking for bias in content exposure (not user demographics)\n",
        "- Identifying risks and mitigation strategies\n",
        "- Clearly documenting system limitations\n",
        "\n",
        "Industry tools like SHAP and LIME are referenced conceptually, but are\n",
        "replaced with recommender-appropriate techniques that match the system design.\n",
        "\"\"\"\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 6a. Explainability ‚Äì Term Overlap Analysis (TF-IDF Inspired)\n",
        "# ---------------------------------------------------------------------------\n",
        "# Purpose: Explain *why* two titles are considered similar using shared\n",
        "# high-importance terms instead of black-box scores.\n",
        "\n",
        "# Conceptual reference: SHAP / LIME\n",
        "# Practical replacement: Inspect which important terms overlap between titles\n",
        "\n",
        "def explain_term_overlap(query_idx, rec_indices, tfidf_matrix, feature_names, top_n=10):\n",
        "    \"\"\"\n",
        "    Identify the most important shared terms that explain why titles\n",
        "    are considered similar.\n",
        "\n",
        "    Purpose:\n",
        "    Provide a clear, human-readable explanation for recommendations\n",
        "    by highlighting overlapping content terms.\n",
        "    \"\"\"\n",
        "    query_vector = tfidf_matrix[query_idx].toarray().flatten()          # Extract TF-IDF values so we can see which terms matter most for the query title.\n",
        "    explanations = []                                                   # Store explanations so results can be reviewed in table form.\n",
        "\n",
        "    for ridx in rec_indices:\n",
        "        rec_vector = tfidf_matrix[ridx].toarray().flatten()             # Extract TF-IDF values for the recommended title for fair comparison.\n",
        "        overlap = query_vector * rec_vector                             # Multiply vectors to isolate shared important terms.\n",
        "        top_terms_idx = overlap.argsort()[::-1][:top_n]                 # Rank shared terms by contribution strength so explanations stay concise.\n",
        "        top_terms = [feature_names[i] for i in top_terms_idx if overlap[i] > 0]  # Keep only terms that actually contribute.\n",
        "\n",
        "        explanations.append({\n",
        "            \"Recommended Index\": ridx,\n",
        "            \"Top Overlapping Terms\": \", \".join(top_terms[:top_n])       # Present terms in readable format for non-technical users.\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(explanations)                                   # Return as a DataFrame for clarity and reporting.\n",
        "\n",
        "\n",
        "query_idx = 0                                                           # Use one example title to demonstrate explainability clearly.\n",
        "rec_indices = get_top_k_indices(query_idx, cosine_sim_matrix, top_k=5)  # Retrieve a small Top-K list to keep explanations focused.\n",
        "\n",
        "term_overlap_df = explain_term_overlap(\n",
        "    query_idx,\n",
        "    rec_indices,\n",
        "    tfidf_matrix,\n",
        "    tfidf.get_feature_names_out(),\n",
        "    top_n=8\n",
        ")\n",
        "\n",
        "print(\"Explainability ‚Äì Term Overlap Analysis\")\n",
        "display(term_overlap_df)\n",
        "print(\n",
        "    f\"INTERPRETATION:\\nThe shared terms shown above explain why the selected \\n\"\n",
        "    f\"title '{df.loc[query_idx, 'title']}' is matched with these recommendations. \\n\"\n",
        "    f\"This mirrors the intent of SHAP or LIME by making similarity decisions \\n\"\n",
        "    f\"transparent and understandable. \\n\"\n",
        ")\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 6b. Explainability ‚Äì Feature Contribution Discussion (Weighted Model)\n",
        "# ---------------------------------------------------------------------------\n",
        "# Purpose: Show how much each content signal contributes so model behavior\n",
        "# can be explained in simple business terms.\n",
        "\n",
        "# Conceptual reference: Feature importance attribution\n",
        "\n",
        "feature_contributions = pd.DataFrame({\n",
        "    \"Feature\": [\"Genre\", \"Description\"],\n",
        "    \"Weight\": [best_genre_weight, 1 - best_genre_weight]\n",
        "})                                                               # Display weights to make trade-offs explicit.\n",
        "\n",
        "print(\"Feature Contribution Discussion (Weighted Model)\")\n",
        "display(feature_contributions)\n",
        "print(\n",
        "    f\"INTERPRETATION:\\nThe model relies more on genre ({best_genre_weight:.2f}) than descriptions ({1 - best_genre_weight:.2f}). \\n\"\n",
        "    f\"This improves interpretability and allows alignment with business preferences. \\n\"\n",
        ")\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 6c. Bias Auditing ‚Äì Genre Exposure Analysis\n",
        "# ---------------------------------------------------------------------------\n",
        "# Purpose: Check whether recommendations disproportionately favor\n",
        "# popular genres compared to the full catalog.\n",
        "\n",
        "# Bias focus: Content exposure, not user demographics\n",
        "\n",
        "\n",
        "# Clean GENRE field to remove NaN or blank (\" \") genre, found this bug during the deck and final report write up\n",
        "df[\"listed_in\"] = (\n",
        "    df[\"listed_in\"]\n",
        "    .fillna(\"\")                    # Remove NaN\n",
        "    .str.lower()\n",
        "    .str.replace(\"&\", \"and\")       # normalize symbol\n",
        "    .str.split(\",\")                # Split multi-label genres\n",
        ")\n",
        "\n",
        "df = df.explode(\"listed_in\")\n",
        "df[\"listed_in\"] = df[\"listed_in\"].str.strip()\n",
        "\n",
        "# Remove empty / invalid genre entries\n",
        "df = df[df[\"listed_in\"] != \"\"]\n",
        "\n",
        "def get_genre_distribution(indices, df):\n",
        "    genres = (\n",
        "        df.loc[indices, \"listed_in\"]\n",
        "        .dropna()                                                    # Prevent missing values from distorting counts.\n",
        "        .astype(str)\n",
        "    )                                                                # Break genres into individual tokens for fair comparison.\n",
        "    return genres.value_counts(normalize=True)                       # Normalize so catalog and recommendations are comparable.\n",
        "\n",
        "catalog_genre_dist = get_genre_distribution(df.index, df)            # Establish baseline genre distribution.\n",
        "recommendation_genre_dist = get_genre_distribution(rec_indices, df)  # Measure genre exposure in recommendations.\n",
        "\n",
        "genre_bias_df = pd.concat(\n",
        "    [catalog_genre_dist, recommendation_genre_dist],\n",
        "    axis=1,\n",
        "    keys=[\"Catalog Distribution\", \"Recommendation Distribution\"]\n",
        ").fillna(0)\n",
        "\n",
        "print(\"Bias Auditing ‚Äì Genre Exposure Analysis: Catalog vs Recommendation Distribution\")\n",
        "display(genre_bias_df.head(10))\n",
        "print(\n",
        "    f\"INTERPRETATION:\\nDifferences between catalog and recommendation \\n\"\n",
        "    f\"genre distributions indicate whether certain genres are overexposed, \\n\"\n",
        "    f\"guiding the need for diversity controls. \\n\"\n",
        ")\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 6d. Bias Auditing ‚Äì Country Exposure Analysis\n",
        "# ---------------------------------------------------------------------------\n",
        "# Purpose: Identify whether recommendations over-represent titles\n",
        "# from dominant production regions.\n",
        "\n",
        "# Bias focus: Regional dominance in recommendations\n",
        "\n",
        "# Clean Country field to remove NaN or blank (\" \") countries, found this bug during the deck and final report write up\n",
        "df[\"country\"] = (\n",
        "    df[\"country\"]\n",
        "    .fillna(\"\")\n",
        "    .str.strip()\n",
        ")\n",
        "\n",
        "# Remove empty countries\n",
        "df = df[df[\"country\"] != \"\"]\n",
        "\n",
        "def get_country_distribution(indices, df):\n",
        "    countries = (\n",
        "        df.loc[indices, \"country\"]\n",
        "        .dropna()                                                        # Prevent missing values from distorting counts.\n",
        "        .astype(str)\n",
        "    )                                                                    # Tokenize country metadata for consistent comparison.\n",
        "    return countries.value_counts(normalize=True)\n",
        "\n",
        "catalog_country_dist = get_country_distribution(df.index, df)            # Baseline regional distribution.\n",
        "recommendation_country_dist = get_country_distribution(rec_indices, df)  # Regional exposure in recommendations.\n",
        "\n",
        "country_bias_df = pd.concat(\n",
        "    [catalog_country_dist, recommendation_country_dist],\n",
        "    axis=1,\n",
        "    keys=[\"Catalog Distribution\", \"Recommendation Distribution\"]\n",
        ").fillna(0)\n",
        "\n",
        "print(\"Bias Auditing ‚Äì Country Exposure: Catalog vs Recommendation Distribution\")\n",
        "display(country_bias_df.head(10))\n",
        "print(\n",
        "    f\"INTERPRETATION:\\nThis table highlights whether recommendations \\n\"\n",
        "    f\"favor titles from specific countries, which may require mitigation \\n\"\n",
        "    f\"to ensure balanced catalog exposure. \\n\"\n",
        ")\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 6e. Fairness Metrics ‚Äì Why Demographic Metrics Are Not Applicable\n",
        "# ---------------------------------------------------------------------------\n",
        "# Purpose: Clearly justify why standard demographic fairness metrics\n",
        "# do not apply to this recommender system.\n",
        "\n",
        "fairness_explanation = pd.DataFrame({\n",
        "    \"Aspect\": [\n",
        "        \"User Demographics\",\n",
        "        \"Protected Attributes\",\n",
        "        \"Prediction Outcomes\",\n",
        "        \"Applicable Fairness Definition\"\n",
        "    ],\n",
        "    \"Status\": [\n",
        "        \"Not Available\",\n",
        "        \"Not Collected\",\n",
        "        \"No Predictions Generated\",\n",
        "        \"Content Exposure & Diversity\"\n",
        "    ]\n",
        "})                                                               # Explicitly define fairness scope to avoid misuse.\n",
        "\n",
        "print(\"Fairness Assessment Scope and Applicability\")\n",
        "display(fairness_explanation)\n",
        "print(\n",
        "    \"INTERPRETATION:\\nSince the system does not model users or predict outcomes, \\n\"\n",
        "    \"fairness is evaluated through balanced content exposure rather than \\n\"\n",
        "    \"demographic parity or similar metrics. \\n\"\n",
        ")\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 6f. Mitigation Strategies (Conceptual + Implemented)\n",
        "# ---------------------------------------------------------------------------\n",
        "# Purpose: Map identified risks to concrete actions already implemented\n",
        "# in the system design.\n",
        "\n",
        "mitigation_strategies = pd.DataFrame({\n",
        "    \"Bias Risk\": [\n",
        "        \"Genre dominance\",\n",
        "        \"Country dominance\",\n",
        "        \"Duplicate-heavy recommendations\",\n",
        "        \"Overly narrow themes\"\n",
        "    ],\n",
        "    \"Mitigation Strategy\": [\n",
        "        \"Intra-list Diversity (ILD)\",\n",
        "        \"Balanced feature weighting\",\n",
        "        \"Exclude near-duplicates\",\n",
        "        \"Weighted hybrid similarity\"\n",
        "    ]\n",
        "})                                                               # Connect risks directly to implemented controls.\n",
        "\n",
        "print(\"Identified Bias Risks and Mitigation Strategies\")\n",
        "display(mitigation_strategies)\n",
        "print(\n",
        "    \"INTERPRETATION:\\nThese mitigation strategies demonstrate proactive \\n\"\n",
        "    \"steps taken to reduce bias and promote fair, diverse recommendations. \\n\"\n",
        ")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Bias Audit Summary Table\n",
        "# ---------------------------------------------------------------------------\n",
        "bias_audit_summary = pd.DataFrame({\n",
        "    \"Bias Risk\": [\n",
        "        \"Genre dominance\",\n",
        "        \"Country dominance\",\n",
        "        \"Overly similar recommendations\",\n",
        "        \"Narrow thematic exposure\"\n",
        "    ],\n",
        "    \"Detection Metric\": [\n",
        "        \"Genre distribution vs catalog baseline\",\n",
        "        \"Country distribution vs catalog baseline\",\n",
        "        \"Intra-list Diversity (ILD)\",\n",
        "        \"Catalog Coverage (CC)\"\n",
        "    ],\n",
        "    \"Mitigation Applied\": [\n",
        "        \"Genre-aware similarity weighting\",\n",
        "        \"Balanced feature weighting\",\n",
        "        \"Intra-list Diversity (ILD)\",\n",
        "        \"Weighted hybrid similarity + coverage-aware selection\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"Bias Audit Summary: Risk, Detection, and Mitigation\")\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "display(bias_audit_summary.style.set_properties(**{\"white-space\": \"pre-wrap\",\"text-align\": \"left\"}))\n",
        "\n",
        "print(\n",
        "    \"INTERPRETATION:\\n\"\n",
        "    \"This summary demonstrates that bias identification, measurement, and mitigation \\n\"\n",
        "    \"are explicitly linked. Each identified exposure risk is paired with an appropriate \\n\"\n",
        "    \"proxy metric and a concrete mitigation strategy already implemented in the system, \\n\"\n",
        "    \"ensuring responsible and auditable recommendation behavior.\\n\"\n",
        ")\n",
        "print(\"\\n\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 6g. Limitations & Honest Disclosure\n",
        "# ---------------------------------------------------------------------------\n",
        "# Purpose: Transparently document system limitations to set realistic\n",
        "# expectations for stakeholders.\n",
        "\n",
        "limitations = pd.DataFrame({\n",
        "    \"Limitation\": [\n",
        "        \"Catalog imbalance\",\n",
        "        \"Potential data leakage\",\n",
        "        \"Generalization to new titles\"\n",
        "    ],\n",
        "    \"Discussion\": [\n",
        "        \"Popular genres and countries may still dominate; diversity metrics \"\n",
        "        \"and weighting help reduce this effect.\",\n",
        "        \"Similarity models use the full catalog; future work could separate \"\n",
        "        \"evaluation data for stronger validation.\",\n",
        "        \"Performance depends on metadata quality and may vary for newly added \"\n",
        "        \"or poorly described titles.\"\n",
        "    ]\n",
        "})                                                               # Explicit disclosure improves trust and auditability.\n",
        "\n",
        "print(\"Model Limitations and Responsible Use Considerations\")\n",
        "display(limitations)\n",
        "print(\n",
        "    \"INTERPRETATION:\\nClearly stating limitations strengthens credibility \\n\"\n",
        "    \"and demonstrates responsible model evaluation. \\n\"\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 6h. Future Work and System Extensions\n",
        "# ---------------------------------------------------------------------------\n",
        "# Purpose: Document realistic and responsible extensions that could enhance\n",
        "# system performance, evaluation rigor, and business impact beyond the\n",
        "# current data-limited scope.\n",
        "\n",
        "print(\"Future Work and System Extensions\")\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\n",
        "    \"This recommender system was intentionally designed as a content-based solution \\n\"\n",
        "    \"to operate under data-limited conditions. While effective for cold-start and \\n\"\n",
        "    \"explainability-driven use cases, several extensions could further improve \\n\"\n",
        "    \"performance and real-world impact.\\n\"\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"First, incorporating user interaction signals such as viewing history, watch \\n\"\n",
        "    \"duration, or implicit feedback would enable a hybrid recommendation approach. \\n\"\n",
        "    \"Combining content similarity with collaborative filtering could improve \\n\"\n",
        "    \"personalization while retaining explainability for new or low-activity users.\\n\"\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"Second, replacing proxy evaluation metrics with online experimentation methods \\n\"\n",
        "    \"such as A/B testing would allow the system to be optimized directly against \\n\"\n",
        "    \"business outcomes. Metrics like click-through rate, completion rate, and session \\n\"\n",
        "    \"duration would provide stronger evidence of real user engagement.\\n\"\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"Finally, continuous monitoring of catalog growth and metadata quality would help \\n\"\n",
        "    \"maintain recommendation performance over time. Periodic retraining and repeated \\n\"\n",
        "    \"bias audits would ensure that the system continues to surface diverse, relevant, \\n\"\n",
        "    \"and representative content as the catalog evolves.\\n\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ìÇÉüñä Key Findings\n",
        "\n",
        "Explainability analysis shows that recommendations are driven by interpretable content signals, using term overlap as a transparent proxy for model reasoning similar to SHAP or LIME-style local explanations. Shared keywords such as ‚Äúfather,‚Äù ‚Äúlife,‚Äù ‚Äúdeath,‚Äù and ‚Äúdocumentaries‚Äù explain why titles like Dick Johnson Is Dead are surfaced, allowing stakeholders to trace recommendations directly to meaningful semantic features. Feature contribution analysis further confirms a balanced weighting between genre (0.40) and description (0.60), ensuring interpretability while preserving semantic richness and business relevance.\n",
        "\n",
        "Bias auditing reveals measurable but controlled exposure differences between the catalog and recommendations. For example, documentaries represent approximately 0.043 of the catalog yet account for 0.600 of recommendations, while international documentary titles appear at 0.400 despite comprising only 0.021 of the catalog. At the country level, U.S. titles account for 0.353 of the catalog and 0.200 of recommendations, while U.K. titles increase from 0.053 to 0.800 for specific anchors. These shifts reflect semantic relevance rather than uncontrolled amplification, indicating that recommendations are driven by content similarity rather than popularity bias.\n",
        "\n",
        "Importantly, diversity controls moderate these effects. Intra-list Diversity remains stable at approximately 0.34, and Catalog Coverage remains near 0.05, indicating that recommendations avoid excessive repetition or collapse toward narrow subsets of content. Together, these results demonstrate that bias is actively monitored and mitigated through explainable feature weighting and diversity-aware selection, supporting transparent, balanced, and responsible recommendation behavior under data-limited conditions.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "J-6ZeLgpdU7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üèÅ Conclusion"
      ],
      "metadata": {
        "id": "IRCM01xpdZA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CONCLUSION\n",
        "# =============================================================================\n",
        "\n",
        "print(\n",
        "    \"FINAL CONCLUSION:\\n\\n\"\n",
        "    \"The solution delivers a robust content-based recommender designed for large catalogs \\n\"\n",
        "    \"and cold-start environments where user interaction data is limited. By balancing \\n\"\n",
        "    \"relevance, diversity, catalog exposure, and explainability, the system produces \\n\"\n",
        "    \"stable, interpretable, and business-aligned recommendations rather than optimizing \\n\"\n",
        "    \"for relevance alone.\\n\\n\"\n",
        "\n",
        "    \"The modeling approach progresses from simple, transparent baselines to semantic \\n\"\n",
        "    \"similarity methods, ensuring each design choice is justified, measurable, and \\n\"\n",
        "    \"auditable. Evaluation, explainability, and bias auditing confirm controlled behavior \\n\"\n",
        "    \"under data-limited conditions and readiness for responsible deployment.\\n\\n\"\n",
        "\n",
        "    \"For Netflix, this approach improves content discovery even when user history is sparse, \\n\"\n",
        "    \"helping new or infrequent viewers find relevant titles faster. By reducing browsing \\n\"\n",
        "    \"time and surfacing diverse recommendations, the system supports viewer engagement \\n\"\n",
        "    \"while avoiding over-reliance on already popular content. The emphasis on explainability \\n\"\n",
        "    \"and bias-aware evaluation also strengthens trust, governance, and long-term scalability, \\n\"\n",
        "    \"providing a solid foundation for future hybrid and personalized recommendation systems.\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "JpQSwCjldVhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "b-lA5qN_c-_3"
      }
    }
  ]
}